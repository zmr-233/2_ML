{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51622e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(tmp1):\n",
    "    import subprocess\n",
    "    import shlex  # 导入 shlex 模块\n",
    "    # 使用 shlex.quote 来转义 inp 字符串\n",
    "    tmp2 = str(tmp1)\n",
    "    safe_str = shlex.quote(tmp2)\n",
    "    subprocess.run('echo {} | wclip'.format(safe_str), shell=True)  \n",
    "\n",
    "def cvin(k):\n",
    "    clip(In[k])\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "#from tqdm import tqdm  # tqdm是显示循环进度条的库\n",
    "from tqdm.notebook import tqdm #推荐在jupyter中使用自带的进度条\n",
    "\n",
    "np.random.seed(0) #重置种子为0\n",
    "\n",
    "import copy #复制方法\n",
    "#格式化输出\n",
    "np.set_printoptions(precision=3, suppress=True, linewidth=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbfec37",
   "metadata": {},
   "source": [
    "#### Q-learning->DQN\n",
    "这种用表格Qsa存储动作价值的做法只在环境的状态和动作都是离散的，并且空间都比较小的情况下适用，比如悬崖漫步，当状态比较大的时候，只能用价值函数：\n",
    "DQN（Deep Q-Network）是Q-learning的一个扩展，它使用了深度学习。从Q-learning到DQN的转换涉及到几个关键的步骤和思想。以下是一个简明的概述：\n",
    "\n",
    "1. 函数逼近器:\n",
    "- Q-learning: 使用表格存储Qsa\n",
    "- DQN: 使用一个神经网络作为函数逼近器来估计Q值(非连续)\n",
    "\n",
    "\n",
    "2. 经验回放 (Experience Replay):\n",
    "- DQN不立即使用每一步的经验来更新其Q值，而是将其存储在一个经验池中。随后，从这个经验池中随机抽取一个批量的经验来更新网络。\n",
    "- 这打破数据之间的相关性，使得学习过程更加稳定\n",
    "\n",
    "3. 目标网络:\n",
    "- Q-learning中，使用当前的Q值估计来更新Q值\n",
    "- DQN中，这会导致学习过程不稳定，因为我们同时在尝试估计和更新同一个网络\n",
    "因此DQN使用了两个网络：一个主网络和一个目标网络(在固定的时间间隔内保持不变）\n",
    "\n",
    "\n",
    "4. 更复杂的神经网络结构:\n",
    "对于图像输入DQN通常使用卷积神经网络 (CNN) 作为其函数逼近器。\n",
    "\n",
    "随着时间的推移，对DQN的许多改进和变种已经被提出：如Double DQN、Prioritized Experience Replay、Dueling DQN等，以进一步提高性能。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ef51cd",
   "metadata": {},
   "source": [
    "## DQN\n",
    "> **通常 DQN（以及 Q-learning）只能处理动作离散的情况，因为在函数的更新过程中有$max_a$这一操作**\n",
    "\n",
    "DQN使用参数化的函数$Q_{\\theta}$来拟合这些数据。很显然，这种函数拟合的方法存在一定的精度损失，因此被称为近似方法。\n",
    "如下用来解决连续状态下离散动作的问题：\n",
    "\n",
    "### 1.CartPole环境\n",
    "状态值就是连续的，动作值是离散的\n",
    "在车杆环境中，有一辆小车，智能体的任务是通过左右移动保持车上的杆竖直：\n",
    "- 若杆的倾斜度数过大\n",
    "- 车子离初始位置左右的偏离程度过大\n",
    "- 坚持时间到达 200 帧\n",
    "\n",
    "则游戏结束。\n",
    "以下是将给定的数据转换为Markdown格式的表格，并将其全部输出在代码块中：\n",
    "\n",
    "- 表 7-1 CartPole环境的状态空间\n",
    "| 维度 | 意义         | 最小值   | 最大值   |\n",
    "|------|--------------|---------|---------|\n",
    "| 0    | 车的位置     | -2.4    | 2.4     |\n",
    "| 1    | 车的速度     | -Inf    | Inf     |\n",
    "| 2    | 杆的角度     | ~ -41.8°| ~ 41.8° |\n",
    "| 3    | 杆尖端的速度 | -Inf    | Inf     |\n",
    "\n",
    "- 表7-2 CartPole环境的动作空间\n",
    "| 标号 | 动作         |\n",
    "|------|--------------|\n",
    "| 0    | 向左移动小车 |\n",
    "| 1    | 向右移动小车 |\n",
    "\n",
    "### 2.DQN解决思路\n",
    "常见的解决方法便是使用 **函数拟合（function approximation）** 的思想：\n",
    "- 若动作无限：输入(s,a)，输出一个标量Q(s,a)\n",
    "- 若动作有限：只输入s，输出每一个动作Q(s,a)\n",
    "\n",
    "设$\\omega$为神经网络用来拟合函数的参数，则表示为：$Q_{\\omega}(s,a)$,我们将用于拟合函数函数的神经网络称为 **Q 网络**\n",
    "\n",
    "**|Q网络损失函数**：从 **时序差分 (temporal difference，TD)** 学习目标 $r+\\gamma \\max _{a^{\\prime} \\in \\mathcal{A}} Q\\left(s^{\\prime}, a^{\\prime}\\right)$ 来增量式更新 $Q(s, a)$ ，也就是说要使 $Q(s, a)$ 和 TD 目标 $r+\\gamma \\max _{a^{\\prime} \\in \\mathcal{A}} Q\\left(s^{\\prime}, a^{\\prime}\\right)$ 靠近。于是，对于一组数据 $\\left\\{\\left(s_{i}, a_{i}, r_{i}, s_{i}^{\\prime}\\right)\\right\\}$ ，我们可以很自然地将 \\mathrm{Q} 网络的损失函数构造为均方误差的形式:\n",
    "$$\n",
    "\\omega^{*}=\\arg \\min _{\\omega} \\frac{1}{2 N} \\sum_{i=1}^{N}\\left[Q_{\\omega}\\left(s_{i}, a_{i}\\right)-\\left(r_{i}+\\gamma \\max _{a^{\\prime}} Q_{\\omega}\\left(s_{i}^{\\prime}, a^{\\prime}\\right)\\right)\\right]^{2}\n",
    "$$\n",
    "\n",
    "就可以将 Q-learning 扩展到神经网络形式—— **深度 Q 网络（deep Q network，DQN）** 算法。由于 DQN 是离线策略算法，同样需要$\\epsilon$-贪婪策略来平衡利用&探索，并把数据收集起来。\n",
    "DQN 中还有两个非常重要的模块——经验回放和目标网络：\n",
    "\n",
    "####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aa5f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1714b73e",
   "metadata": {},
   "source": [
    "#### |附录1：为何均方差使用1/2公式\n",
    "在深度学习和强化学习中，Q网络（或者更广泛地说，神经网络）的损失函数经常包含一个$\\frac{1}{2}$系数。这主要是出于数学上的便利性。\n",
    "\n",
    "为了更好地理解这一点，考虑一个简单的均方误差损失函数：\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y_i})^2\n",
    "$$\n",
    "\n",
    "当我们对其进行微分，以计算关于$\\hat{y_i}$的梯度时，我们得到：\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{y_i}} = \\frac{2}{N} (y_i - \\hat{y_i})\n",
    "$$\n",
    "\n",
    "但是，如果我们在损失函数前面加上一个$\\frac{1}{2}$系数：\n",
    "$$\n",
    "L = \\frac{1}{2N} \\sum_{i=1}^{N} (y_i - \\hat{y_i})^2\n",
    "$$\n",
    "\n",
    "其梯度则为：\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{y_i}} = \\frac{1}{N} (y_i - \\hat{y_i})\n",
    "$$\n",
    "\n",
    "这样，我们得到了一个更加简洁的梯度表达式。尽管乘以$\\frac{1}{2}$并不会改变损失函数的最优值位置，但它可以使梯度的计算更为简单和直观。这在实际的优化中很有用，因为神经网络的训练通常涉及大量的梯度计算。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911622cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
